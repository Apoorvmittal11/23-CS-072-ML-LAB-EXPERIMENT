{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6/GczQySdyaYXnoaymD3S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apoorvmittal11/23-CS-072-ML-LAB-EXPERIMENT/blob/main/23-CS-072%20EXPERIMENT4/23_CS_072_Experiment_4(Naive_Bayes).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load & Preprocess the Dataset"
      ],
      "metadata": {
        "id": "mxIiPbRdPX5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "\n",
        "# Keep only relevant columns\n",
        "df = df[['Category', 'Message']]\n",
        "df.columns = ['label', 'message']\n",
        "\n",
        "# Encode labels (ham = 0, spam = 1)\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['message'], df['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Dataset size:\", df.shape)\n",
        "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp1dDNqUN1cO",
        "outputId": "4a9ee05d-ef83-4d9e-8083-3a9dbbd5cfa0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: (5572, 2)\n",
            "Train size: (4457,) Test size: (1115,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the multinomial Naive Bayes algorithm with Laplace smoothing:\n",
        "\n",
        "P(spam | d) ∝ P(spam) Y\n",
        "w∈d\n",
        "P(w | spam)"
      ],
      "metadata": {
        "id": "tVbUa9mFKRSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "class NaiveBayesScratch:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        self.class_priors = {}\n",
        "        self.word_counts = {}\n",
        "        self.class_totals = {}\n",
        "        self.vocab = set()\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        docs_by_class = defaultdict(list)\n",
        "        for text, label in zip(X, y):\n",
        "            docs_by_class[label].append(text)\n",
        "\n",
        "        total_docs = len(X)\n",
        "        self.class_priors = {\n",
        "            c: len(docs) / total_docs for c, docs in docs_by_class.items()\n",
        "        }\n",
        "\n",
        "        self.word_counts = {c: Counter() for c in docs_by_class}\n",
        "        self.class_totals = {}\n",
        "\n",
        "        for c, docs in docs_by_class.items():\n",
        "            for text in docs:\n",
        "                for word in text.split():\n",
        "                    self.word_counts[c][word] += 1\n",
        "                    self.vocab.add(word)\n",
        "            self.class_totals[c] = sum(self.word_counts[c].values())\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = []\n",
        "        for text in X:\n",
        "            scores = {}\n",
        "            for c in self.class_priors:\n",
        "                score = np.log(self.class_priors[c])\n",
        "                for word in text.split():\n",
        "                    word_count = self.word_counts[c][word] + self.alpha\n",
        "                    total = self.class_totals[c] + self.alpha * len(self.vocab)\n",
        "                    score += np.log(word_count / total)\n",
        "                scores[c] = score\n",
        "            preds.append(max(scores, key=scores.get))\n",
        "        return preds\n"
      ],
      "metadata": {
        "id": "Hp2j1ZTVLtV7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and evaluate Naive Bayes on the same dataset using both CountVectorizer\n",
        "and TfidfVectorizer."
      ],
      "metadata": {
        "id": "Qlb3wnDfKRO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Convert text → Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_cv = vectorizer.fit_transform(X_train)\n",
        "X_test_cv = vectorizer.transform(X_test)\n",
        "\n",
        "# Convert sparse matrix to \"word word word...\" format for scratch model\n",
        "def transform_for_scratch(X, vocab):\n",
        "    inv_vocab = {i: w for w, i in vocab.items()}\n",
        "    docs = []\n",
        "    for row in X:\n",
        "        words = []\n",
        "        for idx, count in zip(row.indices, row.data):\n",
        "            words.extend([inv_vocab[idx]] * count)  # extend list instead of nesting\n",
        "        docs.append(\" \".join(words))\n",
        "    return docs\n",
        "\n",
        "X_train_scratch = transform_for_scratch(X_train_cv, vectorizer.vocabulary_)\n",
        "X_test_scratch = transform_for_scratch(X_test_cv, vectorizer.vocabulary_)\n",
        "\n",
        "# Train model\n",
        "nb = NaiveBayesScratch(alpha=1.0)\n",
        "nb.fit(X_train_scratch, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = nb.predict(X_test_scratch)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f0Dc8mLoLs29"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert text → TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_train_tf = vectorizer_tfidf.fit_transform(X_train)\n",
        "X_test_tf = vectorizer_tfidf.transform(X_test)\n",
        "\n",
        "# Convert TF-IDF to pseudo-counts for scratch model\n",
        "def tfidf_to_counts(X, vocab, scale=100):\n",
        "    inv_vocab = {i: w for w, i in vocab.items()}\n",
        "    docs = []\n",
        "    for row in X:\n",
        "        words = []\n",
        "        for idx, value in zip(row.indices, row.data):\n",
        "            repeat = int(value * scale)  # scale TF-IDF into counts\n",
        "            if repeat > 0:\n",
        "                words.extend([inv_vocab[idx]] * repeat)\n",
        "        docs.append(\" \".join(words))\n",
        "    return docs\n",
        "\n",
        "# TfidfVectorizer conversion\n",
        "X_train_scratch_tf = tfidf_to_counts(X_train_tf, vectorizer_tfidf.vocabulary_)\n",
        "X_test_scratch_tf = tfidf_to_counts(X_test_tf, vectorizer_tfidf.vocabulary_)\n",
        "\n",
        "\n",
        "# Train model\n",
        "nb_tfidf = NaiveBayesScratch(alpha=1.0)\n",
        "nb_tfidf.fit(X_train_scratch_tf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_tf = nb_tfidf.predict(X_test_scratch_tf)"
      ],
      "metadata": {
        "id": "CVLH6LZtOnQU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report accuracy, precision, recall, F1-score, and confusion matrix."
      ],
      "metadata": {
        "id": "ukGiIglXKRLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "print(\"CountVectorizer Results\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1:\", f1_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuOWaS-4Qfio",
        "outputId": "5d41c3b6-4b65-4f36-d8f2-2187a2fa9ae6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer Results\n",
            "Accuracy: 0.9928251121076234\n",
            "Precision: 1.0\n",
            "Recall: 0.9463087248322147\n",
            "F1: 0.9724137931034482\n",
            "Confusion Matrix:\n",
            " [[966   0]\n",
            " [  8 141]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "print(\"\\nTfidfVectorizer Results\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tf))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_tf))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_tf))\n",
        "print(\"F1:\", f1_score(y_test, y_pred_tf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_tf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeVJcdgxQel3",
        "outputId": "4f6d5aa6-69f7-4506-8895-4d7f26b220e0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TfidfVectorizer Results\n",
            "Accuracy: 0.9856502242152466\n",
            "Precision: 0.934640522875817\n",
            "Recall: 0.959731543624161\n",
            "F1: 0.9470198675496688\n",
            "Confusion Matrix:\n",
            " [[956  10]\n",
            " [  6 143]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a results table to summarize your findings. Example format:"
      ],
      "metadata": {
        "id": "o-22Ct7KQukx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# --- Collect metrics from Task 3 and Task 4 ---\n",
        "# CountVectorizer\n",
        "acc_count = accuracy_score(y_test, y_pred)\n",
        "prec_count = precision_score(y_test, y_pred)\n",
        "rec_count = recall_score(y_test, y_pred)\n",
        "f1_count = f1_score(y_test, y_pred)\n",
        "\n",
        "# TF-IDF\n",
        "acc_tfidf = accuracy_score(y_test, y_pred_tf)\n",
        "prec_tfidf = precision_score(y_test, y_pred_tf)\n",
        "rec_tfidf = recall_score(y_test, y_pred_tf)\n",
        "f1_tfidf = f1_score(y_test, y_pred_tf)\n",
        "\n",
        "# --- Build results table ---\n",
        "results = [\n",
        "    [\"Naive Bayes\", \"Count\", 1.0, acc_count, prec_count, rec_count, f1_count],\n",
        "    [\"Naive Bayes\", \"TF-IDF\", 1.0, acc_tfidf, prec_tfidf, rec_tfidf, f1_tfidf]\n",
        "]\n",
        "\n",
        "columns = [\"Model\", \"Vectorizer\", \"Reg. λ\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
        "results_df = pd.DataFrame(results, columns=columns)\n",
        "\n",
        "# Display nicely\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SYQTHeELsOm",
        "outputId": "8c2023af-f74e-43c2-930d-1293561e3017"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Model Vectorizer  Reg. λ  Accuracy  Precision    Recall        F1\n",
            "0  Naive Bayes      Count     1.0  0.992825   1.000000  0.946309  0.972414\n",
            "1  Naive Bayes     TF-IDF     1.0  0.985650   0.934641  0.959732  0.947020\n"
          ]
        }
      ]
    }
  ]
}