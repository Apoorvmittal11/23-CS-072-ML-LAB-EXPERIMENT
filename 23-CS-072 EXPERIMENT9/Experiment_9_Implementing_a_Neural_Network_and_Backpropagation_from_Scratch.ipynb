{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOzPe63Wa8elp8zJaW1RdV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apoorvmittal11/23-CS-072-ML-LAB-EXPERIMENT/blob/main/23-CS-072%20EXPERIMENT9/Experiment_9_Implementing_a_Neural_Network_and_Backpropagation_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 9 : Implementing a Neural Network and Backpropagation from Scratch**"
      ],
      "metadata": {
        "id": "FjN0Xl2hrZV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Learning Objectives**\n",
        "\n",
        "* Upon successful completion of this assignment, students will be able to:\n",
        "Understand and articulate the mathematical foundations of a feedforward neural network.\n",
        "\n",
        "* Implement the core components of an ANN, including parameter initialization, activation\n",
        "functions (ReLU, Sigmoid), and their derivatives.\n",
        "\n",
        "* Implement the Forward Propagation algorithm to generate predictions from network\n",
        "inputs.\n",
        "\n",
        "* Implement the Backpropagation algorithm from scratch to calculate gradients for all\n",
        "network parameters.\n",
        "\n",
        "* Implement various loss functions (Binary Cross-Entropy, Mean Squared Error) and their\n",
        "derivatives.\n",
        "\n",
        "* Implement the Gradient Descent algorithm to update network weights and biases.\n",
        "\n",
        "* Build a complete, modular MyANNClassifier class using only NumPy.\n",
        "\n",
        "* Train the \"from scratch\" classifier on a real-world dataset and evaluate its performance.\n",
        "\n",
        "* Compare the custom-built classifier's performance and behavior against\n",
        "sklearn.neural_network.MLPClassifier .\n",
        "\n",
        "* Analyze the impact of different loss functions and network architectures on model\n",
        "training and final performance."
      ],
      "metadata": {
        "id": "HNUQwED0rZTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Introduction**\n",
        "This assignment is designed to demystify the \"black box\" of neural networks. You will move\n",
        "beyond high-level libraries and implement the core engine of a simple, fully-connected neural\n",
        "network using only NumPy. Your primary task is to build a classifier by implementing the two most critical components: Forward Propagation (for making predictions) and\n",
        "Backpropagation (for learning from errors).\n",
        "\n",
        "You will use the well-known Wisconsin Breast Cancer dataset for a binary classification\n",
        "task. After building your network, you will experiment with different loss functions (BCE vs.\n",
        "MSE) and architectures. Finally, you will compare your \"from scratch\" model to scikit-learn's\n",
        "MLPClassifier to benchmark your work and appreciate the optimizations provided by modern\n",
        "libraries."
      ],
      "metadata": {
        "id": "HaWXLzh2rZP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Prerequisites**\n",
        "\n",
        "Ensure your Python environment has the following libraries installed:"
      ],
      "metadata": {
        "id": "6Kb3yb_frZNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas scikit-learn matplotlib seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiBb1PItsQDW",
        "outputId": "43441ffc-7576-4cbb-9946-6512d28e8a6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Experiment Tasks**\n",
        "\n",
        "You are required to build a complete neural network pipeline. Follow the structured tasks\n",
        "below."
      ],
      "metadata": {
        "id": "uD0yfyBKrZHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1: Data Loading and Preprocessing (15 Marks)**\n",
        "1. Load Data: Load the Breast Cancer Wisconsin dataset directly from scikit-learn.\n",
        "\n",
        "2. Inspect Data: Print the shapes of X and y and the feature names to understand the data.This is a binary classification problem.\n",
        "\n",
        "3. Create Hold-Out Set: Perform a single 70/30 split on the data.\n",
        "\n",
        "* X_train , y_train (70% of the data)\n",
        "\n",
        "* X_val , y_val (30% of the data)\n",
        "\n",
        "* Use train_test_split with random_state=42 for reproducibility.\n",
        "\n",
        "4. Standardize Features: This is critical for neural networks.\n",
        "\n",
        "* Fit a StandardScaler from sklearn.preprocessing on X_train only.\n",
        "\n",
        "* Transform both X_train and X_val using the fitted scaler.\n",
        "\n",
        "* X_train_scaled will be used for training, and X_val_scaled for all final evaluations."
      ],
      "metadata": {
        "id": "j9x4TfbDrZEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1.Load Data\n",
        "data = load_breast_cancer()\n",
        "X = data.data          # features\n",
        "y = data.target        # labels\n",
        "\n",
        "# 2.Inspect Data\n",
        "print(\"Feature matrix shape (X):\", X.shape)\n",
        "print(\"Target vector shape (y):\", y.shape)\n",
        "print(\"\\nFeature names:\\n\", data.feature_names)\n",
        "print(\"\\nTarget names:\", data.target_names)\n",
        "\n",
        "# 3.Create Hold-Out Set (70% Train, 30% Validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nTraining set size:\", X_train.shape)\n",
        "print(\"Validation set size:\", X_val.shape)\n",
        "\n",
        "# 4.Standardize Features\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit only on training data\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform both train and validation sets\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "print(\"\\nAfter scaling:\")\n",
        "print(\"Mean of training features (approx 0):\", np.round(X_train_scaled.mean(), 2))\n",
        "print(\"Std of training features (approx 1):\", np.round(X_train_scaled.std(), 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF1SG9PN01CG",
        "outputId": "e46a6096-ca60-40bf-8031-f9c18a5c368f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix shape (X): (569, 30)\n",
            "Target vector shape (y): (569,)\n",
            "\n",
            "Feature names:\n",
            " ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
            "\n",
            "Target names: ['malignant' 'benign']\n",
            "\n",
            "Training set size: (398, 30)\n",
            "Validation set size: (171, 30)\n",
            "\n",
            "After scaling:\n",
            "Mean of training features (approx 0): -0.0\n",
            "Std of training features (approx 1): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: 'From Scratch' Utilities (NumPy) (20 Marks)**\n",
        "\n",
        "Implement the following helper functions using only NumPy.\n",
        "\n",
        "1. Activation Functions:\n",
        "\n",
        "* sigmoid(Z) : Computes the sigmoid.\n",
        "\n",
        "* relu(Z) : Computes the Rectified Linear Unit ( np.maximum(0, Z) ).\n",
        "\n",
        "2. Activation Derivatives: These are crucial for backpropagation.\n",
        "\n",
        "* sigmoid_derivative(A) : Where A = sigmoid(Z) . The derivative is A * (1 - A) .\n",
        "\n",
        "* relu_derivative(Z) : The derivative is 1 if Z > 0 , and 0 otherwise.\n",
        "\n",
        "3. Loss Functions:\n",
        "\n",
        "* compute_bce_loss(Y, Y_hat) : Computes the Binary Cross-Entropy (BCE) loss. (Add a\n",
        "small epsilon=1e-15 for numerical stability to avoid log(0) ).\n",
        "\n",
        "* compute_mse_loss(Y, Y_hat) : Computes the Mean Squared Error (MSE) loss."
      ],
      "metadata": {
        "id": "UA8LNuXjrZB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# 1.Activation Functions\n",
        "def sigmoid(Z):\n",
        "    \"\"\"Compute the Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"Compute the ReLU activation function.\"\"\"\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "# 2.Activation Derivatives\n",
        "def sigmoid_derivative(A):\n",
        "    \"\"\"\n",
        "    Derivative of Sigmoid.\n",
        "    A = sigmoid(Z)\n",
        "    Derivative = A * (1 - A)\n",
        "    \"\"\"\n",
        "    return A * (1 - A)\n",
        "\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    \"\"\"\n",
        "    Derivative of ReLU.\n",
        "    1 if Z > 0 else 0\n",
        "    \"\"\"\n",
        "    return (Z > 0).astype(float)\n",
        "\n",
        "# 3.Loss Functions\n",
        "def compute_bce_loss(Y, Y_hat):\n",
        "    \"\"\"\n",
        "    Compute Binary Cross-Entropy (BCE) Loss.\n",
        "    Y: True labels (0 or 1)\n",
        "    Y_hat: Predicted probabilities (sigmoid outputs)\n",
        "    \"\"\"\n",
        "    epsilon = 1e-15  # for numerical stability\n",
        "    Y_hat = np.clip(Y_hat, epsilon, 1 - epsilon)\n",
        "    m = Y.shape[0]\n",
        "    loss = - (1 / m) * np.sum(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def compute_mse_loss(Y, Y_hat):\n",
        "    \"\"\"\n",
        "    Compute Mean Squared Error (MSE) Loss.\n",
        "    \"\"\"\n",
        "    m = Y.shape[0]\n",
        "    loss = (1 / (2 * m)) * np.sum((Y - Y_hat) ** 2)\n",
        "    return loss\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    Z = np.array([-1, 0, 1, 2])\n",
        "    A_sig = sigmoid(Z)\n",
        "    A_relu = relu(Z)\n",
        "\n",
        "    print(\"Sigmoid(Z):\", A_sig)\n",
        "    print(\"ReLU(Z):\", A_relu)\n",
        "    print(\"Sigmoid Derivative:\", sigmoid_derivative(A_sig))\n",
        "    print(\"ReLU Derivative:\", relu_derivative(Z))\n",
        "\n",
        "    Y = np.array([1, 0, 1, 0])\n",
        "    Y_hat = np.array([0.9, 0.1, 0.8, 0.3])\n",
        "    print(\"BCE Loss:\", compute_bce_loss(Y, Y_hat))\n",
        "    print(\"MSE Loss:\", compute_mse_loss(Y, Y_hat))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo7bQNXV01mt",
        "outputId": "8f5e022b-6ace-4f29-adbd-7a332836cfb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid(Z): [0.26894142 0.5        0.73105858 0.88079708]\n",
            "ReLU(Z): [0 0 1 2]\n",
            "Sigmoid Derivative: [0.19661193 0.25       0.19661193 0.10499359]\n",
            "ReLU Derivative: [0. 0. 1. 1.]\n",
            "BCE Loss: 0.19763488164214868\n",
            "MSE Loss: 0.018749999999999996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3: 'From Scratch' ANN Classifier (40 Marks)**\n",
        "\n",
        "Implement a MyANNClassifier class. This class will orchestrate the entire learning process.\n",
        "\n",
        "1. Class Structure ( __init__ ):\n",
        "\n",
        "* __init__(self, layer_dims, learning_rate=0.01, n_iterations=1000, loss='bce') :\n",
        "\n",
        "  * layer_dims : A list specifying the number of units in each layer. e.g., [n_x, 10, 5, 1] , where n_x is the number of input features (30 for the breast cancer dataset).\n",
        "\n",
        "  * Store learning_rate , n_iterations , and loss (either 'bce' or 'mse').\n",
        "\n",
        "  * self.parameters_ : A dictionary to store weights ( W1 , W2 , ...) and biases ( b1 , b2 ,...).\n",
        "\n",
        "  * self.costs_ : A list to store the loss at each iteration (for plotting).\n",
        "\n",
        "2. Parameter Initialization ( _initialize_parameters ):\n",
        "\n",
        "* Create a helper method that iterates through layer_dims .\n",
        "* Initialize weights W with small random values ( np.random.randn(...) * 0.01 ) to break\n",
        "symmetry.\n",
        "* Initialize biases b as zeros ( np.zeros(...) ).\n",
        "* Store them in self.parameters_ (e.g., self.parameters_['W1'] , self.parameters_['b1'] ).\n",
        "3. Forward Propagation ( _forward_propagation ):\n",
        "\n",
        "* Create a method _forward_propagation(self, X) .\n",
        "\n",
        "* A_prev = X .\n",
        "\n",
        "* Loop from layer 1 to L:\n",
        "\n",
        "  * The hidden layers (1 to L-1) must use the ReLU activation.\n",
        "\n",
        "  * The output layer (L) must use the Sigmoid activation (for binary classification).\n",
        "\n",
        "  * Calculate Z = W @ A_prev + b .\n",
        "\n",
        "  * Calculate A = activation(Z) .\n",
        "\n",
        "  * Store all A (activations) and Z (linear results) in a cache (e.g., a list of tuples(A, Z) ). This cache is essential for backpropagation.\n",
        "\n",
        "* Return the final activation A_L (which is Y_hat ) and the cache .\n",
        "\n",
        "4. Backward Propagation ( _backward_propagation ):\n",
        "\n",
        "* Create a method _backward_propagation(self, Y, Y_hat, cache) . This is the most complex\n",
        "task.\n",
        "\n",
        "* Y is the true labels, Y_hat is the prediction ( A_L ) from the forward pass.\n",
        "\n",
        "* Initialize Backprop:\n",
        "  * Calculate dA_L (the derivative of the loss function w.r.t. Y_hat ).\n",
        "    * If self.loss == 'bce' : dA_L = -(np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n",
        "    * If self.loss == 'mse' : dA_L = 2 * (Y_hat - Y)\n",
        "  * Output Layer (Sigmoid):\n",
        "    * Get A_L and Z_L from the cache .\n",
        "    * dZ_L = dA_L * sigmoid_derivative(A_L)\n",
        "    * Calculate dW_L and db_L using dZ_L and the corresponding A_prev from the cache.\n",
        "  * Loop Backwards (Hidden Layers - ReLU):\n",
        "    * Iterate from layer L-1 down to 1.\n",
        "    * Calculate dA_prev = W.T @ dZ (using W and dZ from the current layer).\n",
        "    * dZ_prev = dA_prev * relu_derivative(Z_prev) (using Z_prev from the cache).\n",
        "    * Calculate dW and db for this layer.\n",
        "  * Store all gradients ( dW1 , db1 , dW2 , db2 , ...) in a grads dictionary.\n",
        "5. Parameter Update ( _update_parameters ):\n",
        "\n",
        "* Create a method _update_parameters(self, grads) .\n",
        "\n",
        "* Iterate through all parameters in self.parameters_ .\n",
        "\n",
        "* Update them using gradient descent:\n",
        "\n",
        "  * W = W - self.learning_rate * dW\n",
        "\n",
        "  * b = b - self.learning_rate * db\n",
        "\n",
        "6. Fit and Predict Methods:\n",
        "\n",
        "* fit(self, X, y) :\n",
        "\n",
        "  * Reshape y to be (1, n_samples) .\n",
        "  * Reshape X to be (n_features, n_samples) .\n",
        "  * Call _initialize_parameters .\n",
        "  * Loop for n_iterations :\n",
        "      1. Y_hat, cache = _forward_propagation(X)\n",
        "      2. loss = compute_bce_loss(y, Y_hat) (or mse based on self.loss )\n",
        "      3. grads = _backward_propagation(y, Y_hat, cache)\n",
        "      4. _update_parameters(grads)\n",
        "      5. Store the loss in self.costs_ .\n",
        "* predict(self, X) :\n",
        "\n",
        "  * Reshape X to (n_features, n_samples) .\n",
        "  * Run _forward_propagation(X) to get Y_hat .\n",
        "  * Convert probabilities to binary predictions: predictions = (Y_hat > 0.5).astype(int) .\n",
        "  * Return the flattened 1D array of predictions."
      ],
      "metadata": {
        "id": "k2-t832XrY_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class MyANNClassifier:\n",
        "    def __init__(self, layer_dims, learning_rate=0.01, n_iterations=1000, loss='bce'):\n",
        "        \"\"\"\n",
        "        Initialize the ANN model.\n",
        "        layer_dims: list of layer sizes [n_input, n_hidden1, ..., n_output]\n",
        "        \"\"\"\n",
        "        self.layer_dims = layer_dims\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.loss = loss\n",
        "        self.parameters_ = {}\n",
        "        self.costs_ = []\n",
        "\n",
        "    # 1.Parameter Initialization\n",
        "    def _initialize_parameters(self):\n",
        "        np.random.seed(42)  # for reproducibility\n",
        "        L = len(self.layer_dims)\n",
        "\n",
        "        for l in range(1, L):\n",
        "            self.parameters_[f\"W{l}\"] = np.random.randn(\n",
        "                self.layer_dims[l], self.layer_dims[l-1]\n",
        "            ) * 0.01\n",
        "            self.parameters_[f\"b{l}\"] = np.zeros((self.layer_dims[l], 1))\n",
        "\n",
        "    # 2.Forward Propagation\n",
        "    def _forward_propagation(self, X):\n",
        "        cache = []  # to store (A, Z)\n",
        "        A = X\n",
        "        L = len(self.layer_dims) - 1\n",
        "\n",
        "        for l in range(1, L + 1):\n",
        "            W = self.parameters_[f\"W{l}\"]\n",
        "            b = self.parameters_[f\"b{l}\"]\n",
        "            Z = np.dot(W, A) + b\n",
        "\n",
        "            if l < L:\n",
        "                A = relu(Z)\n",
        "            else:\n",
        "                A = sigmoid(Z)\n",
        "\n",
        "            cache.append((A, Z))\n",
        "\n",
        "        return A, cache  # A = Y_hat\n",
        "\n",
        "    # 3.Backward Propagation\n",
        "    def _backward_propagation(self, Y, Y_hat, cache):\n",
        "        grads = {}\n",
        "        L = len(cache)\n",
        "        m = Y.shape[1]\n",
        "\n",
        "        # Compute dA for output layer\n",
        "        if self.loss == 'bce':\n",
        "            dA = -(np.divide(Y, Y_hat + 1e-15) - np.divide(1 - Y, 1 - Y_hat + 1e-15))\n",
        "        else:  # mse\n",
        "            dA = 2 * (Y_hat - Y)\n",
        "\n",
        "        # Output layer (Sigmoid)\n",
        "        A_L, Z_L = cache[-1]\n",
        "        dZ = dA * sigmoid_derivative(A_L)\n",
        "        A_prev = cache[-2][0] if L > 1 else None\n",
        "        grads[f\"dW{L}\"] = (1/m) * np.dot(dZ, A_prev.T if A_prev is not None else Y.T)\n",
        "        grads[f\"db{L}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "        # Hidden layers (ReLU)\n",
        "        for l in reversed(range(1, L)):\n",
        "            A, Z = cache[l-1]\n",
        "            W_next = self.parameters_[f\"W{l+1}\"]\n",
        "            dA_prev = np.dot(W_next.T, dZ)\n",
        "            dZ = dA_prev * relu_derivative(Z)\n",
        "            A_prev = cache[l-2][0] if l > 1 else None\n",
        "            grads[f\"dW{l}\"] = (1/m) * np.dot(dZ, A_prev.T if A_prev is not None else Y.T)\n",
        "            grads[f\"db{l}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "        return grads\n",
        "\n",
        "    # 4.Update Parameters\n",
        "    def _update_parameters(self, grads):\n",
        "        L = len(self.layer_dims) - 1\n",
        "        for l in range(1, L + 1):\n",
        "            self.parameters_[f\"W{l}\"] -= self.learning_rate * grads[f\"dW{l}\"]\n",
        "            self.parameters_[f\"b{l}\"] -= self.learning_rate * grads[f\"db{l}\"]\n",
        "\n",
        "    # 5.Fit Model\n",
        "    def fit(self, X, y):\n",
        "        X = X.T  # shape: (n_features, n_samples)\n",
        "        y = y.reshape(1, -1)\n",
        "        self._initialize_parameters()\n",
        "\n",
        "        for i in range(self.n_iterations):\n",
        "            Y_hat, cache = self._forward_propagation(X)\n",
        "\n",
        "            # Compute loss\n",
        "            if self.loss == 'bce':\n",
        "                cost = compute_bce_loss(y.flatten(), Y_hat.flatten())\n",
        "            else:\n",
        "                cost = compute_mse_loss(y.flatten(), Y_hat.flatten())\n",
        "\n",
        "            grads = self._backward_propagation(y, Y_hat, cache)\n",
        "            self._update_parameters(grads)\n",
        "            self.costs_.append(cost)\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Iteration {i}, Loss: {cost:.6f}\")\n",
        "\n",
        "    # 6.Predict\n",
        "    def predict(self, X):\n",
        "        X = X.T\n",
        "        Y_hat, _ = self._forward_propagation(X)\n",
        "        predictions = (Y_hat > 0.5).astype(int)\n",
        "        return predictions.flatten()\n"
      ],
      "metadata": {
        "id": "5qXMEvLD02sH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyANNClassifier(layer_dims=[30, 10, 5, 1], learning_rate=0.01, n_iterations=1000, loss='bce')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = model.predict(X_val_scaled)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"\\nValidation Accuracy:\", accuracy_score(y_val, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb9AJVL35JoC",
        "outputId": "64750937-4c76-4519-d4b9-0b07e57ad79d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 0.693145\n",
            "Iteration 100, Loss: 0.680709\n",
            "Iteration 200, Loss: 0.673160\n",
            "Iteration 300, Loss: 0.668563\n",
            "Iteration 400, Loss: 0.665753\n",
            "Iteration 500, Loss: 0.664029\n",
            "Iteration 600, Loss: 0.662968\n",
            "Iteration 700, Loss: 0.662312\n",
            "Iteration 800, Loss: 0.661907\n",
            "Iteration 900, Loss: 0.661655\n",
            "\n",
            "Validation Accuracy: 0.631578947368421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4: Training and Experimentation (15 Marks)**\n",
        "\n",
        "Use your scaled training and validation sets ( X_train_scaled , y_train , X_val_scaled , y_val ).\n",
        "\n",
        "1. Model 1 (BCE Loss):\n",
        "\n",
        "* Define your layer_dims . Start with one hidden layer (e.g., [30, 10, 1] ).\n",
        "\n",
        "* Instantiate MyANNClassifier with loss='bce' , learning_rate=0.001 , and n_iterations=5000 .\n",
        "\n",
        "* fit the model on X_train_scaled and y_train .\n",
        "\n",
        "* predict on X_val_scaled .\n",
        "\n",
        "* Print the classification_report for this model.\n",
        "\n",
        "2. Model 2 (MSE Loss):\n",
        "\n",
        "* Instantiate a new model with the exact same parameters as Model 1, but set\n",
        "loss='mse' .\n",
        "\n",
        "* fit and predict as before.\n",
        "\n",
        "* Print the classification_report for this model.\n",
        "\n",
        "3. Model 3 (Deeper Architecture):\n",
        "\n",
        "* Instantiate a new model with loss='bce' but a deeper architecture (e.g., [30, 10, 5, 1] ).\n",
        "\n",
        "* fit and predict .\n",
        "\n",
        "* Print the classification_report for this model."
      ],
      "metadata": {
        "id": "1MLFyJAnrY8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Model 1 — BCE Loss (Single Hidden Layer)\n",
        "print(\"Model 1: BCE Loss (Single Hidden Layer)\")\n",
        "\n",
        "layer_dims_1 = [30, 10, 1]  # input layer = 30 features, 1 hidden layer (10 neurons), output = 1\n",
        "model1 = MyANNClassifier(layer_dims=layer_dims_1,\n",
        "                         learning_rate=0.001,\n",
        "                         n_iterations=5000,\n",
        "                         loss='bce')\n",
        "\n",
        "model1.fit(X_train_scaled, y_train)\n",
        "y_pred1 = model1.predict(X_val_scaled)\n",
        "\n",
        "print(\"\\nClassification Report — Model 1 (BCE):\")\n",
        "print(classification_report(y_val, y_pred1))\n",
        "\n",
        "# Model 2 — MSE Loss (Single Hidden Layer)\n",
        "print(\"\\nModel 2: MSE Loss (Single Hidden Layer)\")\n",
        "\n",
        "layer_dims_2 = [30, 10, 1]\n",
        "model2 = MyANNClassifier(layer_dims=layer_dims_2,\n",
        "                         learning_rate=0.001,\n",
        "                         n_iterations=5000,\n",
        "                         loss='mse')\n",
        "\n",
        "model2.fit(X_train_scaled, y_train)\n",
        "y_pred2 = model2.predict(X_val_scaled)\n",
        "\n",
        "print(\"\\nClassification Report — Model 2 (MSE):\")\n",
        "print(classification_report(y_val, y_pred2))\n",
        "\n",
        "# Model 3 — BCE Loss (Deeper Architecture)\n",
        "print(\"\\nModel 3: BCE Loss (Deeper Architecture)\")\n",
        "\n",
        "layer_dims_3 = [30, 10, 5, 1]  # deeper network with 2 hidden layers\n",
        "model3 = MyANNClassifier(layer_dims=layer_dims_3,\n",
        "                         learning_rate=0.001,\n",
        "                         n_iterations=5000,\n",
        "                         loss='bce')\n",
        "\n",
        "model3.fit(X_train_scaled, y_train)\n",
        "y_pred3 = model3.predict(X_val_scaled)\n",
        "\n",
        "print(\"\\nClassification Report — Model 3 (Deeper BCE):\")\n",
        "print(classification_report(y_val, y_pred3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI-HghhJ17AY",
        "outputId": "41da5c2a-4e8d-427b-ecde-9e4a5932658d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1: BCE Loss (Single Hidden Layer)\n",
            "Iteration 0, Loss: 0.693180\n",
            "Iteration 100, Loss: 0.691634\n",
            "Iteration 200, Loss: 0.690164\n",
            "Iteration 300, Loss: 0.688766\n",
            "Iteration 400, Loss: 0.687436\n",
            "Iteration 500, Loss: 0.686172\n",
            "Iteration 600, Loss: 0.684969\n",
            "Iteration 700, Loss: 0.683826\n",
            "Iteration 800, Loss: 0.682738\n",
            "Iteration 900, Loss: 0.681702\n",
            "Iteration 1000, Loss: 0.680716\n",
            "Iteration 1100, Loss: 0.679778\n",
            "Iteration 1200, Loss: 0.678886\n",
            "Iteration 1300, Loss: 0.678037\n",
            "Iteration 1400, Loss: 0.677230\n",
            "Iteration 1500, Loss: 0.676462\n",
            "Iteration 1600, Loss: 0.675731\n",
            "Iteration 1700, Loss: 0.675035\n",
            "Iteration 1800, Loss: 0.674373\n",
            "Iteration 1900, Loss: 0.673743\n",
            "Iteration 2000, Loss: 0.673144\n",
            "Iteration 2100, Loss: 0.672574\n",
            "Iteration 2200, Loss: 0.672032\n",
            "Iteration 2300, Loss: 0.671516\n",
            "Iteration 2400, Loss: 0.671025\n",
            "Iteration 2500, Loss: 0.670557\n",
            "Iteration 2600, Loss: 0.670111\n",
            "Iteration 2700, Loss: 0.669687\n",
            "Iteration 2800, Loss: 0.669284\n",
            "Iteration 2900, Loss: 0.668901\n",
            "Iteration 3000, Loss: 0.668536\n",
            "Iteration 3100, Loss: 0.668188\n",
            "Iteration 3200, Loss: 0.667858\n",
            "Iteration 3300, Loss: 0.667545\n",
            "Iteration 3400, Loss: 0.667247\n",
            "Iteration 3500, Loss: 0.666964\n",
            "Iteration 3600, Loss: 0.666695\n",
            "Iteration 3700, Loss: 0.666442\n",
            "Iteration 3800, Loss: 0.666201\n",
            "Iteration 3900, Loss: 0.665972\n",
            "Iteration 4000, Loss: 0.665755\n",
            "Iteration 4100, Loss: 0.665550\n",
            "Iteration 4200, Loss: 0.665355\n",
            "Iteration 4300, Loss: 0.665170\n",
            "Iteration 4400, Loss: 0.664995\n",
            "Iteration 4500, Loss: 0.664830\n",
            "Iteration 4600, Loss: 0.664673\n",
            "Iteration 4700, Loss: 0.664526\n",
            "Iteration 4800, Loss: 0.664387\n",
            "Iteration 4900, Loss: 0.664255\n",
            "\n",
            "Classification Report — Model 1 (BCE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        63\n",
            "           1       0.63      1.00      0.77       108\n",
            "\n",
            "    accuracy                           0.63       171\n",
            "   macro avg       0.32      0.50      0.39       171\n",
            "weighted avg       0.40      0.63      0.49       171\n",
            "\n",
            "\n",
            "Model 2: MSE Loss (Single Hidden Layer)\n",
            "Iteration 0, Loss: 0.125008\n",
            "Iteration 100, Loss: 0.124813\n",
            "Iteration 200, Loss: 0.124622\n",
            "Iteration 300, Loss: 0.124436\n",
            "Iteration 400, Loss: 0.124254\n",
            "Iteration 500, Loss: 0.124077\n",
            "Iteration 600, Loss: 0.123905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 700, Loss: 0.123737\n",
            "Iteration 800, Loss: 0.123573\n",
            "Iteration 900, Loss: 0.123413\n",
            "Iteration 1000, Loss: 0.123257\n",
            "Iteration 1100, Loss: 0.123105\n",
            "Iteration 1200, Loss: 0.122957\n",
            "Iteration 1300, Loss: 0.122813\n",
            "Iteration 1400, Loss: 0.122672\n",
            "Iteration 1500, Loss: 0.122535\n",
            "Iteration 1600, Loss: 0.122401\n",
            "Iteration 1700, Loss: 0.122270\n",
            "Iteration 1800, Loss: 0.122143\n",
            "Iteration 1900, Loss: 0.122019\n",
            "Iteration 2000, Loss: 0.121898\n",
            "Iteration 2100, Loss: 0.121780\n",
            "Iteration 2200, Loss: 0.121665\n",
            "Iteration 2300, Loss: 0.121553\n",
            "Iteration 2400, Loss: 0.121443\n",
            "Iteration 2500, Loss: 0.121337\n",
            "Iteration 2600, Loss: 0.121233\n",
            "Iteration 2700, Loss: 0.121132\n",
            "Iteration 2800, Loss: 0.121033\n",
            "Iteration 2900, Loss: 0.120937\n",
            "Iteration 3000, Loss: 0.120843\n",
            "Iteration 3100, Loss: 0.120751\n",
            "Iteration 3200, Loss: 0.120662\n",
            "Iteration 3300, Loss: 0.120575\n",
            "Iteration 3400, Loss: 0.120490\n",
            "Iteration 3500, Loss: 0.120408\n",
            "Iteration 3600, Loss: 0.120327\n",
            "Iteration 3700, Loss: 0.120248\n",
            "Iteration 3800, Loss: 0.120172\n",
            "Iteration 3900, Loss: 0.120097\n",
            "Iteration 4000, Loss: 0.120024\n",
            "Iteration 4100, Loss: 0.119953\n",
            "Iteration 4200, Loss: 0.119884\n",
            "Iteration 4300, Loss: 0.119817\n",
            "Iteration 4400, Loss: 0.119751\n",
            "Iteration 4500, Loss: 0.119687\n",
            "Iteration 4600, Loss: 0.119624\n",
            "Iteration 4700, Loss: 0.119563\n",
            "Iteration 4800, Loss: 0.119504\n",
            "Iteration 4900, Loss: 0.119446\n",
            "\n",
            "Classification Report — Model 2 (MSE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        63\n",
            "           1       0.63      1.00      0.77       108\n",
            "\n",
            "    accuracy                           0.63       171\n",
            "   macro avg       0.32      0.50      0.39       171\n",
            "weighted avg       0.40      0.63      0.49       171\n",
            "\n",
            "\n",
            "Model 3: BCE Loss (Deeper Architecture)\n",
            "Iteration 0, Loss: 0.693145\n",
            "Iteration 100, Loss: 0.691605\n",
            "Iteration 200, Loss: 0.690140\n",
            "Iteration 300, Loss: 0.688746\n",
            "Iteration 400, Loss: 0.687421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500, Loss: 0.686160\n",
            "Iteration 600, Loss: 0.684961\n",
            "Iteration 700, Loss: 0.683820\n",
            "Iteration 800, Loss: 0.682734\n",
            "Iteration 900, Loss: 0.681702\n",
            "Iteration 1000, Loss: 0.680720\n",
            "Iteration 1100, Loss: 0.679785\n",
            "Iteration 1200, Loss: 0.678896\n",
            "Iteration 1300, Loss: 0.678050\n",
            "Iteration 1400, Loss: 0.677246\n",
            "Iteration 1500, Loss: 0.676480\n",
            "Iteration 1600, Loss: 0.675751\n",
            "Iteration 1700, Loss: 0.675058\n",
            "Iteration 1800, Loss: 0.674398\n",
            "Iteration 1900, Loss: 0.673771\n",
            "Iteration 2000, Loss: 0.673173\n",
            "Iteration 2100, Loss: 0.672605\n",
            "Iteration 2200, Loss: 0.672064\n",
            "Iteration 2300, Loss: 0.671549\n",
            "Iteration 2400, Loss: 0.671059\n",
            "Iteration 2500, Loss: 0.670592\n",
            "Iteration 2600, Loss: 0.670148\n",
            "Iteration 2700, Loss: 0.669725\n",
            "Iteration 2800, Loss: 0.669323\n",
            "Iteration 2900, Loss: 0.668940\n",
            "Iteration 3000, Loss: 0.668575\n",
            "Iteration 3100, Loss: 0.668228\n",
            "Iteration 3200, Loss: 0.667898\n",
            "Iteration 3300, Loss: 0.667583\n",
            "Iteration 3400, Loss: 0.667283\n",
            "Iteration 3500, Loss: 0.666998\n",
            "Iteration 3600, Loss: 0.666726\n",
            "Iteration 3700, Loss: 0.666468\n",
            "Iteration 3800, Loss: 0.666221\n",
            "Iteration 3900, Loss: 0.665987\n",
            "Iteration 4000, Loss: 0.665763\n",
            "Iteration 4100, Loss: 0.665550\n",
            "Iteration 4200, Loss: 0.665348\n",
            "Iteration 4300, Loss: 0.665155\n",
            "Iteration 4400, Loss: 0.664971\n",
            "Iteration 4500, Loss: 0.664796\n",
            "Iteration 4600, Loss: 0.664629\n",
            "Iteration 4700, Loss: 0.664470\n",
            "Iteration 4800, Loss: 0.664318\n",
            "Iteration 4900, Loss: 0.664174\n",
            "\n",
            "Classification Report — Model 3 (Deeper BCE):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        63\n",
            "           1       0.63      1.00      0.77       108\n",
            "\n",
            "    accuracy                           0.63       171\n",
            "   macro avg       0.32      0.50      0.39       171\n",
            "weighted avg       0.40      0.63      0.49       171\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5: Comparison with scikit-learn (10 Marks)**\n",
        "\n",
        "1. Train MLPClassifier :\n",
        "\n",
        "* Import from sklearn.neural_network import MLPClassifier .\n",
        "\n",
        "* Instantiate MLPClassifier with parameters that roughly match your best \"from scratch\"\n",
        "model.\n",
        "\n",
        "* Example: MLPClassifier(hidden_layer_sizes=(10,), activation='relu', solver='adam', max_iter=1000,\n",
        "learning_rate_init=0.001, random_state=42) .\n",
        "\n",
        "* fit the MLPClassifier on X_train_scaled and y_train .\n",
        "\n",
        "2. Evaluate MLPClassifier :\n",
        "\n",
        "* predict on X_val_scaled .\n",
        "\n",
        "* Print the classification_report for the sklearn model."
      ],
      "metadata": {
        "id": "aqeCvrv0rY53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW_sjt6mrXbJ",
        "outputId": "38557f92-9948-4fcd-bb32-e316d4dda4e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 4: Scikit-Learn MLPClassifier\n",
            "\n",
            "Classification Report — Model 4 (scikit-learn MLPClassifier):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98        63\n",
            "           1       0.99      0.99      0.99       108\n",
            "\n",
            "    accuracy                           0.99       171\n",
            "   macro avg       0.99      0.99      0.99       171\n",
            "weighted avg       0.99      0.99      0.99       171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Model 4: Scikit-Learn MLPClassifier\")\n",
        "\n",
        "# Match parameters similar to your best model (say, Model 1 or 3)\n",
        "mlp_model = MLPClassifier(\n",
        "    hidden_layer_sizes=(10,),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=1000,\n",
        "    learning_rate_init=0.001,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train\n",
        "mlp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_mlp = mlp_model.predict(X_val_scaled)\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nClassification Report — Model 4 (scikit-learn MLPClassifier):\")\n",
        "print(classification_report(y_val, y_pred_mlp))\n"
      ]
    }
  ]
}